{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "RRc7rBv02kd7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jaY_2UC2kd3"
      },
      "source": [
        "# **Sources**\n",
        "* IMDB Dataset: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "l42kDNGeEO_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRc7rBv02kd7"
      },
      "source": [
        "# **Data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "url = \"/content/drive/MyDrive/Temp/Datasets/IMDB Dataset.csv\""
      ],
      "metadata": {
        "id": "H0AuPcLH3GC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXdiN53-2kd8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Y"
      ],
      "metadata": {
        "id": "_Qg_yh_LGW_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "Y = le.fit_transform(df.sentiment)\n",
        "CLASSES = le.classes_"
      ],
      "metadata": {
        "id": "zWb6QOStGQP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing X"
      ],
      "metadata": {
        "id": "8PUnB9RdGNbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "5cYn4e3xG-_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "id": "zDwA6nWNGfyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "9Si2QmaJGQNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import string\n",
        "punc = string.punctuation\n",
        "abbv = {\n",
        "    \"AFAIK\":\"as far as I know\",\n",
        "\t\"IMO\":\t\"in my opinion\",\n",
        "\t\"IMHO\":\t\"in my humble opinion\",\n",
        "\t\"LGTM\":\t\"look good to me\",\n",
        "\t\"AKA\":\t\"also know as\",\n",
        "\t\"ASAP\":\t\"as sone as possible\",\n",
        "\t\"BTW\":\t\"by the way\",\n",
        "\t\"FAQ\":\t\"frequently asked questions\",\n",
        "\t\"DIY\":\t\"do it yourself\",\n",
        "\t\"DM\":\t\"direct message\",\n",
        "\t\"FYI\":\t\"for your information\",\n",
        "\t\"IC\":\t\"i see\",\n",
        "\t\"IOW\":\t\"in other words\",\n",
        "\t\"IIRC\":\t\"If I Remember Correctly\",\n",
        "\t\"icymi\":\"In case you missed it\",\n",
        "\t\"CUZ\":\t\"because\",\n",
        "\t\"COS\":\t\"because\",\n",
        "\t\"nv\":\t\"nevermind\",\n",
        "\t\"PLZ\":\t\"please\",\n",
        "}\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')\n",
        "\n",
        "import re\n",
        "html_pattern = re.compile('<.*?>')\n",
        "urls_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "\tu\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "\tu\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "\tu\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "\tu\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "\"]+\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # HTML Tags\n",
        "    text = html_pattern.sub(r'', text)\n",
        "\n",
        "    # urls\n",
        "    text = urls_pattern.sub(r'', text)\n",
        "\n",
        "    # punctuations\n",
        "    text = text.translate(str.maketrans(\"\", \"\", punc))\n",
        "\n",
        "    # Emojis\n",
        "    text = emoji.demojize(text)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    new_text = []\n",
        "\n",
        "    for word in text.split(\" \"):\n",
        "\n",
        "        # abbreviations\n",
        "        word = abbv.get(word.upper(), word)\n",
        "            \n",
        "        # Stemming\n",
        "        word = ps.stem(word)\n",
        "\n",
        "        new_text.append(word)\n",
        "\n",
        "    text = \" \".join(new_text)\n",
        "\n",
        "    return text\n",
        "\n",
        "preprocess(\"This is the best movie I have ever watched\")"
      ],
      "metadata": {
        "id": "jfxm6pxuGc8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# # cleaned = df.review.apply(preprocess)\n",
        "\n",
        "# cleaned = []\n",
        "# for i in tqdm(df.review):\n",
        "#     cleaned.append(preprocess(i))"
      ],
      "metadata": {
        "id": "XUXjFn2qHj5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# WRITTING\n",
        "# with open(\"/content/drive/MyDrive/Temp/dumps/cleaned_reviews1.json\", 'w') as f:\n",
        "#     json.dump(cleaned, f)\n",
        "\n",
        "# READING\n",
        "with open(\"/content/drive/MyDrive/Temp/dumps/cleaned_reviews1.json\", 'rb') as f:\n",
        "    cleaned = json.load(f)"
      ],
      "metadata": {
        "id": "4TsWJsc_Ila5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting"
      ],
      "metadata": {
        "id": "rX2zk4SfHM1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cleaned_train, cleaned_test, Y_train, Y_test = train_test_split(\n",
        "\tcleaned,\n",
        "\tY,\n",
        "\ttest_size=0.2,\n",
        "\trandom_state=42,\n",
        "\tstratify=Y\n",
        ")"
      ],
      "metadata": {
        "id": "pSgpHX8DJFJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyper Parameters**"
      ],
      "metadata": {
        "id": "hBy1TcqnEmla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 5000\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "MAXLEN = 2400"
      ],
      "metadata": {
        "id": "34yhUvzqErRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word to Vector**"
      ],
      "metadata": {
        "id": "s4ZvrwqTDKjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_train[0]"
      ],
      "metadata": {
        "id": "H3Nuax5-DIDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing"
      ],
      "metadata": {
        "id": "k2uInjcuGDvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing and Fitting Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "\tnum_words = VOCAB_SIZE,\t\t\t# vocab size\n",
        "\toov_token = OOV_TOKEN,\t    \t# word out of vocab to replace with\n",
        "\tfilters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "\tlower = True,\n",
        ")\n",
        "tokenizer.fit_on_texts(cleaned_train)"
      ],
      "metadata": {
        "id": "fdo4ava6DSs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tokens = tokenizer.texts_to_sequences(cleaned_train)\n",
        "X_test_tokens = tokenizer.texts_to_sequences(cleaned_test)"
      ],
      "metadata": {
        "id": "3yj6jLSlJXFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding"
      ],
      "metadata": {
        "id": "2pAg9NMQI8CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_tokens, maxlen=MAXLEN)\n",
        "X_test_padded = pad_sequences(X_test_tokens, maxlen=MAXLEN)"
      ],
      "metadata": {
        "id": "muTSzProJEhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting to Tensors"
      ],
      "metadata": {
        "id": "_qaf7846GBKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((X_train_padded, Y_train))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test_padded, Y_test))"
      ],
      "metadata": {
        "id": "miE_Ja3yELQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffle | Batch | Pad"
      ],
      "metadata": {
        "id": "5AgWqZ4hKysp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Shuffle the training data\n",
        "train_data = train_data.shuffle(BUFFER_SIZE)\n",
        "test_data = test_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Batch and pad the datasets to the maximum length of the sequences\n",
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "test_data = test_data.padded_batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "7tD30l7SK2v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "3VCbXJVpGPmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "obELXGEmGSwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D, Conv1D, Bidirectional, LSTM"
      ],
      "metadata": {
        "id": "y2mgHPqRGpm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM = 16\n",
        "LSTM_DIM = 12\n",
        "FILTERS = 16\n",
        "KERNEL_SIZE = 8\n",
        "EPOCHS = 5\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(MAXLEN,)),\n",
        "    Embedding(VOCAB_SIZE, EMB_DIM),\n",
        "    Conv1D(filters=FILTERS, kernel_size=KERNEL_SIZE, activation='relu'),\n",
        "    # GlobalAveragePooling1D(),\n",
        "    Bidirectional(LSTM(LSTM_DIM)),\n",
        "    Dense(80, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "with tf.device('/GPU:0'):\n",
        "    history = model.fit(train_data, epochs=EPOCHS, validation_data=test_data).history"
      ],
      "metadata": {
        "id": "qBiQk_t-GAG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting"
      ],
      "metadata": {
        "id": "CQdCjAKrLoyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "C6Pral7QHAL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize = (15, 5))\n",
        "# fig.subplots_adjust(hspace=10, wspace=10)\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(history['accuracy'])\n",
        "axes[0].plot(history['val_accuracy'])\n",
        "axes[0].set_title(\"Accuracy\")\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(history['loss'])\n",
        "axes[1].plot(history['val_loss'])\n",
        "axes[1].set_title(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d0eJFNuaMEyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Realtime Testing"
      ],
      "metadata": {
        "id": "38fskmDQOOwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"not a good movie at all\"\n",
        "\n",
        "cleaned_text = preprocess(text)\n",
        "token_text = tokenizer.texts_to_sequences([cleaned_text])\n",
        "padded_text = pad_sequences(token_text, maxlen=MAXLEN)\n",
        "pred = model.predict(padded_text)[0, 0]\n",
        "\n",
        "label = CLASSES[round(pred)]\n",
        "probability = pred\n",
        "\n",
        "print(label, probability)"
      ],
      "metadata": {
        "id": "nqgldD70OQJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQxS6EVJO1kS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}